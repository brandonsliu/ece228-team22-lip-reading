{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import resize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import torchvision.io\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for Lip Reading\n",
    "class LipReadingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        if '.DS_Store' in self.classes:\n",
    "            self.classes.remove('.DS_Store')\n",
    "        \n",
    "        for label, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name, 'train')\n",
    "            video_files = glob.glob(os.path.join(class_dir, \"*.mp4\"))\n",
    "            self.video_paths.extend(video_files)\n",
    "            self.labels.extend([label] * len(video_files))\n",
    "\n",
    "        # Debug prints\n",
    "        print(f\"Found {len(self.video_paths)} videos across {len(self.classes)} classes.\")\n",
    "        if len(self.video_paths) == 0:\n",
    "            print(\"No videos found. Please check the dataset directory structure and paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        frames = self.load_video_frames(video_path)\n",
    "\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "\n",
    "        return frames, label\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale\n",
    "            frame = Image.fromarray(frame)  # Convert numpy array to PIL Image\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for video frames\n",
    "class ToTensor:\n",
    "    def __call__(self, frames):\n",
    "        tensor = torch.stack([transforms.ToTensor()(frame) for frame in frames])  \n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 videos across 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Path to the processed_selected_mp4_files directory\n",
    "root_dir = './processed_selected_mp4_files'  # Update this path\n",
    "\n",
    "# Dataset and DataLoader\n",
    "transform = ToTensor()\n",
    "train_dataset = LipReadingDataset(root_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: collate_fn(x))\n",
    "\n",
    "# Collate function to handle variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    videos, labels = zip(*batch)\n",
    "    max_len = max(len(video) for video in videos)\n",
    "    padded_videos = []\n",
    "    for video in videos:\n",
    "        assert len(video) == max_len\n",
    "        \n",
    "        # Not needed since all videos are same length(same number of frames). Should all be 29 frames\n",
    "        # padded_video = torch.cat([video, torch.zeros((pad_size, video.shape[1], video.shape[2], video.shape[3]))], dim=0)  # Add padding if num frames < max_len\n",
    "        \n",
    "        # Resize video to (max_len, 1, 224, 224)\n",
    "        resized_video = torch.stack([resize(frame, (128, 128)) for frame in video])\n",
    "        padded_videos.append(resized_video)\n",
    "\n",
    "        # padded_videos.append(video)\n",
    "\n",
    "    return torch.stack(padded_videos), torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture using VGG\n",
    "class LipReadingModel(nn.Module):\n",
    "    def __init__(self, num_classes=500):\n",
    "        super(LipReadingModel, self).__init__()\n",
    "        # VGG16 as feature extractor\n",
    "        self.vgg = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
    "        # self.vgg.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)  # Modify first conv layer for grayscale\n",
    "\n",
    "        self.vgg.classifier = nn.Identity()  # Remove final classification layer\n",
    "\n",
    "        # RNN for sequence modeling\n",
    "        self.rnn = nn.LSTM(input_size=512*4*4, hidden_size=256, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(256*2, num_classes)  # bidirectional doubles the output features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        # print(c_in.shape)\n",
    "        c_out = self.vgg.features(c_in)\n",
    "        # print(c_out.shape)\n",
    "        c_out = c_out.view(batch_size, timesteps, -1)  # Flatten for LSTM\n",
    "        # print(c_out.shape)\n",
    "        r_out, _ = self.rnn(c_out)\n",
    "        # print(r_out.shape)\n",
    "        out = self.fc(r_out[:, -1, :])\n",
    "        # print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6faab35b43334846b0878585009f6678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/20 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Average Loss: 0.7938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3739d0848b4be38f9719997904e863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/20 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Average Loss: 0.7247\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842bb3608ebe4ae9a38743f42beebe19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/20 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Average Loss: 0.7805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e04e6220c32405cb5e7db0d46613311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/20 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Average Loss: 0.7137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796403f3e92c4510bd8be9ff5d8ee80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/20 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Average Loss: 0.6965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5a8ef9b6514384a15226ffdbf5de94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/20 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Average Loss: 0.7570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638b97e96e06429fa3297e46ead7d0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/20 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Average Loss: 0.7001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f87374a18c74aa8ad2795ff2a936bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/20 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Average Loss: 0.7157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1943199e7e914366b7fcfc21fef93226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/20 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Average Loss: 0.6970\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5295121c1b234848bcd8479fbab5dcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/20 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Average Loss: 0.6960\n",
      "Model saved to vgg16_lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = len(train_dataset.classes)  # Automatically get the number of classes\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 1\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = LipReadingModel(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with progress bar\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\")\n",
    "    for i, (videos, labels) in enumerate(progress_bar):\n",
    "        videos = videos.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = 'vgg16_lstm_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from vgg16_lstm_model.pth\n",
      "Test Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# To run saved model on test dataset\n",
    "\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# # Define the path where the model is saved\n",
    "# model_save_path = 'vgg16_lstm_model.pth'\n",
    "# # Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# num_classes = len(train_dataset.classes)\n",
    "# # Instantiate the model\n",
    "# model = LipReadingModel(num_classes=num_classes).to(device)\n",
    "\n",
    "# # Load the state dictionary\n",
    "# model.load_state_dict(torch.load(model_save_path))\n",
    "# model.eval()  # Set the model to evaluation mode\n",
    "# print(\"Model loaded from\", model_save_path)\n",
    "\n",
    "# # Function to evaluate the model\n",
    "# def evaluate_model(model, data_loader):\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     all_labels = []\n",
    "#     all_preds = []\n",
    "\n",
    "#     with torch.no_grad():  # No need to track gradients for inference\n",
    "#         for videos, labels in data_loader:\n",
    "#             videos = videos.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             outputs = model(videos)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "#     # Calculate accuracy\n",
    "#     accuracy = accuracy_score(all_labels, all_preds)\n",
    "#     return accuracy\n",
    "\n",
    "# # Example of using the loaded model for inference on the test dataset\n",
    "# test_accuracy = evaluate_model(model, test_loader)\n",
    "# print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE228",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
