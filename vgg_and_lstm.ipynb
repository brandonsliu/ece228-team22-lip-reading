{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import resize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import torchvision.io\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for Lip Reading\n",
    "class LipReadingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        if '.DS_Store' in self.classes:\n",
    "            self.classes.remove('.DS_Store')\n",
    "        \n",
    "        for label, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name, 'train')\n",
    "            video_files = glob.glob(os.path.join(class_dir, \"*.mp4\"))\n",
    "            self.video_paths.extend(video_files)\n",
    "            self.labels.extend([label] * len(video_files))\n",
    "\n",
    "        # Debug prints\n",
    "        print(f\"Found {len(self.video_paths)} videos across {len(self.classes)} classes.\")\n",
    "        if len(self.video_paths) == 0:\n",
    "            print(\"No videos found. Please check the dataset directory structure and paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        frames = self.load_video_frames(video_path)\n",
    "\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "\n",
    "        return frames, label\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale\n",
    "            frame = Image.fromarray(frame)  # Convert numpy array to PIL Image\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for video frames\n",
    "class ToTensor:\n",
    "    def __call__(self, frames):\n",
    "        tensor = torch.stack([transforms.ToTensor()(frame) for frame in frames])  \n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 videos across 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Path to the processed_selected_mp4_files directory\n",
    "root_dir = './processed_selected_mp4_files'  # Update this path\n",
    "\n",
    "# Dataset and DataLoader\n",
    "transform = ToTensor()\n",
    "train_dataset = LipReadingDataset(root_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=lambda x: collate_fn(x))\n",
    "\n",
    "# Collate function to handle variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    videos, labels = zip(*batch)\n",
    "    max_len = max(len(video) for video in videos)\n",
    "    padded_videos = []\n",
    "    for video in videos:\n",
    "        pad_size = max_len - len(video)\n",
    "        # Ensure the tensor dimensions match for concatenation\n",
    "\n",
    "        padded_video = torch.cat([video, torch.zeros((pad_size, video.shape[1], video.shape[2], video.shape[3]))], dim=0)  # Add padding for grayscale frames\n",
    "        \n",
    "        # Resize video to (max_len, 1, 224, 224)\n",
    "        resized_video = torch.stack([resize(frame, (224, 224)) for frame in padded_video])\n",
    "        padded_videos.append(resized_video)\n",
    "        # padded_videos.append(padded_video)\n",
    "    return torch.stack(padded_videos), torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture using VGG\n",
    "class LipReadingModel(nn.Module):\n",
    "    def __init__(self, num_classes=500):\n",
    "        super(LipReadingModel, self).__init__()\n",
    "        # VGG16 as feature extractor\n",
    "        self.vgg = torchvision.models.vgg16(weights='DEFAULT')\n",
    "        self.vgg.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)  # Modify first conv layer for grayscale\n",
    "\n",
    "        self.vgg.classifier = nn.Identity()  # Remove final classification layer\n",
    "\n",
    "        # RNN for sequence modeling\n",
    "        self.rnn = nn.LSTM(input_size=512*7*7, hidden_size=256, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(256*2, num_classes)  # bidirectional doubles the output features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        c_out = self.vgg.features(c_in)\n",
    "        c_out = c_out.view(batch_size, timesteps, -1)  # Flatten for LSTM\n",
    "        r_out, _ = self.rnn(c_out)\n",
    "        out = self.fc(r_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04afbbbabb9d4e399f1b2e1998d13d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Average Loss: 0.7823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed03687d58e4cfa9749b5a29259f6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Average Loss: 0.7630\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e27e3e7ba2044b1a28b2972543fdf7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Average Loss: 0.7145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca961d2af5444a8bf455f6b547c6e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Average Loss: 0.6950\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f9b703033a42bca8ece2f406bbb491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = len(train_dataset.classes)  # Automatically get the number of classes\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = LipReadingModel(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with progress bar\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\")\n",
    "    for i, (videos, labels) in enumerate(progress_bar):\n",
    "        videos = videos.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE228",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
