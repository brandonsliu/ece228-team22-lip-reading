{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import resize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import torchvision.io\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for Lip Reading\n",
    "class LipReadingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        if '.DS_Store' in self.classes:\n",
    "            self.classes.remove('.DS_Store')\n",
    "        \n",
    "        for label, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name, 'train')\n",
    "            video_files = glob.glob(os.path.join(class_dir, \"*.mp4\"))\n",
    "            self.video_paths.extend(video_files)\n",
    "            self.labels.extend([label] * len(video_files))\n",
    "\n",
    "        # Debug prints\n",
    "        print(f\"Found {len(self.video_paths)} videos across {len(self.classes)} classes.\")\n",
    "        if len(self.video_paths) == 0:\n",
    "            print(\"No videos found. Please check the dataset directory structure and paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        frames = self.load_video_frames(video_path)\n",
    "\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "\n",
    "        return frames, label\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale\n",
    "            frame = Image.fromarray(frame)  # Convert numpy array to PIL Image\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for video frames\n",
    "class ToTensor:\n",
    "    def __call__(self, frames):\n",
    "        tensor = torch.stack([transforms.ToTensor()(frame) for frame in frames])  \n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 videos across 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Path to the processed_selected_mp4_files directory\n",
    "root_dir = './processed_selected_mp4_files'  # Update this path\n",
    "\n",
    "# Dataset and DataLoader\n",
    "transform = ToTensor()\n",
    "train_dataset = LipReadingDataset(root_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=lambda x: collate_fn(x))\n",
    "test_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=lambda x: collate_fn(x))\n",
    "\n",
    "# Collate function to handle variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    videos, labels = zip(*batch)\n",
    "    max_len = max(len(video) for video in videos)\n",
    "    padded_videos = []\n",
    "    for video in videos:\n",
    "        pad_size = max_len - len(video)\n",
    "        # Ensure the tensor dimensions match for concatenation\n",
    "\n",
    "        padded_video = torch.cat([video, torch.zeros((pad_size, video.shape[1], video.shape[2], video.shape[3]))], dim=0)  # Add padding for grayscale frames\n",
    "        \n",
    "        # Resize video to (max_len, 1, 224, 224)\n",
    "        resized_video = torch.stack([resize(frame, (224, 224)) for frame in padded_video])\n",
    "        padded_videos.append(resized_video)\n",
    "        # padded_videos.append(padded_video)\n",
    "    return torch.stack(padded_videos), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.cnn = models.resnet18(weights=\"DEFAULT\")\n",
    "        self.cnn.conv1 = nn.Conv2d(1, self.cnn.conv1.out_channels, kernel_size=self.cnn.conv1.kernel_size, \n",
    "                                   stride=self.cnn.conv1.stride, padding=self.cnn.conv1.padding, bias=False)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-2])  # Remove the classification layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.size()\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        features = self.cnn(x)\n",
    "        features = features.view(batch_size, seq_length, -1)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, cnn_model, hidden_dim, num_classes, num_layers=2):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.cnn = cnn_model\n",
    "        self.lstm = nn.LSTM(input_size=512 * 7 * 7, hidden_size=hidden_dim, num_layers=num_layers, batch_first=False, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            cnn_features = self.cnn(x)\n",
    "        lstm_out, _ = self.lstm(cnn_features)\n",
    "        output = self.fc(lstm_out[:, -1, :])  # Use the output from the last LSTM cell\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.8249804377555847\n",
      "Epoch 2/10, Loss: 0.8183687329292297\n",
      "Epoch 3/10, Loss: 0.6991655230522156\n",
      "Epoch 4/10, Loss: 0.6558952331542969\n",
      "Epoch 5/10, Loss: 0.6106643676757812\n",
      "Epoch 6/10, Loss: 0.6167622804641724\n",
      "Epoch 7/10, Loss: 0.5625695586204529\n",
      "Epoch 8/10, Loss: 0.5704450011253357\n",
      "Epoch 9/10, Loss: 0.5360334515571594\n",
      "Epoch 10/10, Loss: 0.5357653498649597\n",
      "Model saved to resnet_lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, criterion, optimizer, scheduler, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "num_classes = len(train_dataset.classes)  # Automatically get the number of classes\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "cnn_model = CNNFeatureExtractor().to(device)\n",
    "model = CNNLSTM(cnn_model, hidden_dim=256, num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "\n",
    "dataloader = train_loader\n",
    "\n",
    "train(model, dataloader, criterion, optimizer, scheduler, num_epochs=num_epochs)\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = 'resnet_lstm_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 10 / 20 correct (50.00)\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(loader, model):\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        return 100 * acc\n",
    "\n",
    "acc = check_accuracy(test_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
