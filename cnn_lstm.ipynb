{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import resize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import torchvision.io\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for Lip Reading\n",
    "class LipReadingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        if '.DS_Store' in self.classes:\n",
    "            self.classes.remove('.DS_Store')\n",
    "        \n",
    "        for label, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name, 'train')\n",
    "            video_files = glob.glob(os.path.join(class_dir, \"*.mp4\"))\n",
    "            self.video_paths.extend(video_files)\n",
    "            self.labels.extend([label] * len(video_files))\n",
    "\n",
    "        # Debug prints\n",
    "        print(f\"Found {len(self.video_paths)} videos across {len(self.classes)} classes.\")\n",
    "        if len(self.video_paths) == 0:\n",
    "            print(\"No videos found. Please check the dataset directory structure and paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        frames = self.load_video_frames(video_path)\n",
    "\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "\n",
    "        return frames, label\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale\n",
    "            frame = Image.fromarray(frame)  # Convert numpy array to PIL Image\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for video frames\n",
    "class ToTensor:\n",
    "    def __call__(self, frames):\n",
    "        tensor = torch.stack([transforms.ToTensor()(frame) for frame in frames])  \n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 videos across 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Path to the processed_selected_mp4_files directory\n",
    "root_dir = './processed_selected_mp4_files'  # Update this path\n",
    "\n",
    "# Dataset and DataLoader\n",
    "transform = ToTensor()\n",
    "train_dataset = LipReadingDataset(root_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=lambda x: collate_fn(x))\n",
    "\n",
    "# Collate function to handle variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    videos, labels = zip(*batch)\n",
    "    max_len = max(len(video) for video in videos)\n",
    "    padded_videos = []\n",
    "    for video in videos:\n",
    "        pad_size = max_len - len(video)\n",
    "        # Ensure the tensor dimensions match for concatenation\n",
    "\n",
    "        padded_video = torch.cat([video, torch.zeros((pad_size, video.shape[1], video.shape[2], video.shape[3]))], dim=0)  # Add padding for grayscale frames\n",
    "        \n",
    "        # Resize video to (max_len, 1, 224, 224)\n",
    "        resized_video = torch.stack([resize(frame, (224, 224)) for frame in padded_video])\n",
    "        padded_videos.append(resized_video)\n",
    "        # padded_videos.append(padded_video)\n",
    "    return torch.stack(padded_videos), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.cnn = models.resnet18(weights=\"DEFAULT\")\n",
    "        self.cnn.conv1 = nn.Conv2d(1, self.cnn.conv1.out_channels, kernel_size=self.cnn.conv1.kernel_size, \n",
    "                                   stride=self.cnn.conv1.stride, padding=self.cnn.conv1.padding, bias=False)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-2])  # Remove the classification layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.size()\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        features = self.cnn(x)\n",
    "        features = features.view(batch_size, seq_length, -1)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, cnn_model, hidden_dim, num_classes, num_layers=1):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.cnn = cnn_model\n",
    "        self.lstm = nn.LSTM(input_size=512 * 7 * 7, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            cnn_features = self.cnn(x)\n",
    "        lstm_out, _ = self.lstm(cnn_features)\n",
    "        output = self.fc(lstm_out[:, -1, :])  # Use the output from the last LSTM cell\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zachary\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.5678398609161377\n",
      "Epoch 2/20, Loss: 0.6310233473777771\n",
      "Epoch 3/20, Loss: 0.5345368385314941\n",
      "Epoch 4/20, Loss: 0.4503503739833832\n",
      "Epoch 5/20, Loss: 0.4365369379520416\n",
      "Epoch 6/20, Loss: 0.3823126256465912\n",
      "Epoch 7/20, Loss: 0.3784024715423584\n",
      "Epoch 8/20, Loss: 0.3330995440483093\n",
      "Epoch 9/20, Loss: 0.2866179645061493\n",
      "Epoch 10/20, Loss: 0.2673974335193634\n",
      "Epoch 11/20, Loss: 0.21023885905742645\n",
      "Epoch 12/20, Loss: 0.2609831988811493\n",
      "Epoch 13/20, Loss: 0.21097242832183838\n",
      "Epoch 14/20, Loss: 0.16426411271095276\n",
      "Epoch 15/20, Loss: 0.14461857080459595\n",
      "Epoch 16/20, Loss: 0.11699937283992767\n",
      "Epoch 17/20, Loss: 0.09704429656267166\n",
      "Epoch 18/20, Loss: 0.11190438270568848\n",
      "Epoch 19/20, Loss: 0.0853935033082962\n",
      "Epoch 20/20, Loss: 0.0870797410607338\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "num_classes = len(train_dataset.classes)  # Automatically get the number of classes\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "batch_size = 10\n",
    "\n",
    "cnn_model = CNNFeatureExtractor().to(device)\n",
    "model = CNNLSTM(cnn_model, hidden_dim=256, num_classes=len(train_dataset.classes)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "dataloader = train_loader\n",
    "\n",
    "train(model, dataloader, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
