{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import resize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import torchvision.io\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for Lip Reading\n",
    "class LipReadingDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True, less_data=False, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        if '.DS_Store' in self.classes:\n",
    "            self.classes.remove('.DS_Store')\n",
    "        \n",
    "        t_set = (\"train\" if train == True else \"test\")\n",
    "        \n",
    "        self.classes = self.classes[:15]\n",
    "        for label, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name, t_set)\n",
    "            video_files = glob.glob(os.path.join(class_dir, \"*.mp4\"))\n",
    "            self.video_paths.extend(video_files)\n",
    "            self.labels.extend([label] * len(video_files))\n",
    "\n",
    "        if less_data:\n",
    "            self.video_paths = self.video_paths[::2]\n",
    "            self.labels = self.labels[::2]\n",
    "\n",
    "\n",
    "        # Debug prints\n",
    "        print(f\"Found {len(self.video_paths)} videos across {len(self.classes)} classes.\")\n",
    "        if len(self.video_paths) == 0:\n",
    "            print(\"No videos found. Please check the dataset directory structure and paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        frames = self.load_video_frames(video_path)\n",
    "\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "\n",
    "        return frames, label\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale\n",
    "            frame = Image.fromarray(frame)  # Convert numpy array to PIL Image\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for video frames\n",
    "class ToTensor:\n",
    "    def __call__(self, frames):\n",
    "        tensor = torch.stack([transforms.ToTensor()(frame) for frame in frames])  \n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 750 videos across 15 classes.\n",
      "Found 375 videos across 15 classes.\n"
     ]
    }
   ],
   "source": [
    "# Path to the processed_selected_mp4_files directory\n",
    "root_dir = '/Users/Zachary/Documents/Courses/ECE228/project/processed_selected_mp4_files'  # Update this path\n",
    "\n",
    "# Dataset and DataLoader\n",
    "transform = ToTensor()\n",
    "train_dataset = LipReadingDataset(root_dir, train=True, less_data=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=lambda x: collate_fn(x))\n",
    "test_dataset = LipReadingDataset(root_dir, train=False, less_data=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True, collate_fn=lambda x: collate_fn(x))\n",
    "\n",
    "# Collate function to handle variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    videos, labels = zip(*batch)\n",
    "    max_len = max(len(video) for video in videos)\n",
    "    padded_videos = []\n",
    "    for video in videos:\n",
    "        pad_size = max_len - len(video)\n",
    "        padded_video = torch.cat([video, torch.zeros((pad_size, video.shape[1], video.shape[2], video.shape[3]))], dim=0)\n",
    "        resized_video = torch.stack([resize(frame, (224, 224)) for frame in padded_video])\n",
    "        padded_videos.append(resized_video)\n",
    "    return torch.stack(padded_videos), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.cnn = models.resnet50(weights=\"DEFAULT\")\n",
    "        self.cnn.conv1 = nn.Conv2d(1, self.cnn.conv1.out_channels, kernel_size=self.cnn.conv1.kernel_size, \n",
    "                                   stride=self.cnn.conv1.stride, padding=self.cnn.conv1.padding, bias=False)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-2])  # Remove the classification layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.size()\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        features = self.cnn(x)\n",
    "        features = features.view(batch_size, seq_length, -1)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, cnn_model, hidden_dim, num_classes, num_layers=2):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        self.cnn = cnn_model\n",
    "        self.lstm = nn.LSTM(input_size=2048 * 7 * 7, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=8, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            cnn_features = self.cnn(x)\n",
    "        lstm_out, _ = self.lstm(cnn_features)\n",
    "        attention_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        output = self.fc(attention_out[:, -1, :])  # Use the output from the last LSTM cell\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d6e18f693844d8a24ba466e62ab987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/75 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 5.9460, LR: 0.009755\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980931f7e0e543c7ad41ae26f2552249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/75 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 2.8175, LR: 0.009045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3098a2576bbb40849545e368544a45a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/75 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 2.7561, LR: 0.007939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46c46fc192344099f020b05996d3511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/75 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 2.7541, LR: 0.006545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cada87019173402cadaae15bea0bba4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/75 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 2.7476, LR: 0.005000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e6308b6d574ee38857bb30e35dae66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/75 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 2.7016, LR: 0.003455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70179e5cee624caca35d2ca9f458184f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/75 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 2.6685, LR: 0.002061\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b239e19cd7ef4d8bb291afd850811fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/75 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 2.6034, LR: 0.000955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c494d3279dc44d781053693bfe1fbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/75 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 2.5337, LR: 0.000245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b49698cf4d4460bd3c985773764742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/75 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 2.4709, LR: 0.000000\n",
      "Model saved to resnet_lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, criterion, optimizer, scheduler, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_progress_bar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            train_progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "num_classes = len(train_dataset.classes)  # Automatically get the number of classes\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "cnn_model = CNNFeatureExtractor().to(device)\n",
    "model = AttentionLSTM(cnn_model, hidden_dim=256, num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, scheduler, num_epochs=num_epochs)\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = 'resnet_lstm_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 47 / 375 correct (0.13)\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(loader, model):\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)  # move to device, e.g. GPU\n",
    "            y = y.to(device)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, acc))\n",
    "        return acc\n",
    "\n",
    "acc = check_accuracy(test_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
